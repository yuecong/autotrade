{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] This non-commercial license of GraphLab Create is assigned to yuecong1104@gmail.comand will expire on August 13, 2016. For commercial licensing options, visit https://dato.com/buy/.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-14259 - Server binary: /home/cyue/anaconda/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1440461372.log\n",
      "[INFO] GraphLab Server Version: 1.5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int,int,int,int,int,int,float,float,float,float,float,float,float,float,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2013-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Read 3135 lines. Lines per second: 139831\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2003-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2005-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2002-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2009-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2014-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2010-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2006-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2012-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2007-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2013-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2003-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2015-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2005-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2008-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2015-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2008-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2004-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2006-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2011-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2004-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2002-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2009-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2012-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2011-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2007-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2014-07-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Finished parsing file /home/cyue/autotrade/price_EUR_USD_2010-01-01T00%3A00%3A00Z_H1.csv\n",
      "PROGRESS: Parsing completed. Parsed 69296 lines in 0.359264 secs.\n"
     ]
    }
   ],
   "source": [
    "filename = \"./price_EUR_USD_*H1.csv\"\n",
    "sf_eur_usd_h1 = gl.SFrame.read_csv(filename, verbose =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "gl_logger = logging.getLogger('graphlab')\n",
    "gl_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_h1 = sf_eur_usd_h1.add_column( sf_eur_usd_h1['year'].astype(str) + '-' + \n",
    "                                          sf_eur_usd_h1['month'].astype(str) + '-' + \n",
    "                                         sf_eur_usd_h1['day'].astype(str) , 'date' )\n",
    "sf_eur_usd_h1 = sf_eur_usd_h1.add_column( sf_eur_usd_h1['year'].astype(str)+ '-' + \n",
    "                                         sf_eur_usd_h1['month'].astype(str) + '-' + \n",
    "                                         sf_eur_usd_h1['day'].astype(str) +':' +\n",
    "                                         sf_eur_usd_h1['hour'].astype(str), 'date-hour' )\n",
    "sf_eur_usd_h1 = sf_eur_usd_h1.add_column((sf_eur_usd_h1['openBid'] + sf_eur_usd_h1['openAsk']) /2 , 'open')\n",
    "sf_eur_usd_h1 = sf_eur_usd_h1.add_column((sf_eur_usd_h1['highBid'] + sf_eur_usd_h1['highAsk']) /2 , 'high')\n",
    "sf_eur_usd_h1 = sf_eur_usd_h1.add_column((sf_eur_usd_h1['lowBid'] + sf_eur_usd_h1['lowAsk']) /2 , 'low')\n",
    "sf_eur_usd_h1 = sf_eur_usd_h1.add_column((sf_eur_usd_h1['closeBid'] + sf_eur_usd_h1['closeAsk']) /2 , 'close')\n",
    "sf_eur_usd_h1 = sf_eur_usd_h1.remove_columns(['openBid','openAsk','highBid','highAsk','lowBid','lowAsk','closeBid','closeAsk','complete'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_h1 = sf_eur_usd_h1.sort(['year','month','day','hour','minute','second'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40595, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf_eur_usd_h1[sf_eur_usd_h1['volume'] >1000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sf_eur_usd_h1 = sf_eur_usd_h1[sf_eur_usd_h1['volume'] >1000]\n",
    "sf_eur_usd_h1_tmp = sf_eur_usd_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reconstruc_volume():\n",
    "    normal_volume_mean = sf_eur_usd_h1_tmp[sf_eur_usd_h1_tmp['volume'] >5]['volume'].mean()\n",
    "    volume_list = sf_eur_usd_h1_tmp['volume']\n",
    "    new_volume_list = volume_list.apply(lambda x: normal_volume_mean if x<=5 else x)\n",
    "    sf_eur_usd_h1_tmp['volume'] = new_volume_list.astype(int)\n",
    "reconstruc_volume()\n",
    "sf_eur_usd_h1= sf_eur_usd_h1_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_h1_tmp = sf_eur_usd_h1_tmp.add_column(sf_eur_usd_h1_tmp['open'] *sf_eur_usd_h1_tmp['volume'] ,'open_volume')\n",
    "sf_eur_usd_h1_tmp = sf_eur_usd_h1_tmp.add_column(sf_eur_usd_h1_tmp['high'] *sf_eur_usd_h1_tmp['volume'] ,'high_volume')\n",
    "sf_eur_usd_h1_tmp = sf_eur_usd_h1_tmp.add_column(sf_eur_usd_h1_tmp['low'] *sf_eur_usd_h1_tmp['volume'] ,'low_volume')\n",
    "sf_eur_usd_h1_tmp = sf_eur_usd_h1_tmp.add_column(sf_eur_usd_h1_tmp['close'] *sf_eur_usd_h1_tmp['volume'] ,'close_volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_d1_tmp = sf_eur_usd_h1_tmp.groupby(['day','month','year','date'],\n",
    "                                              [gl.aggregate.SUM('open_volume'),\n",
    "                                               gl.aggregate.SUM('high_volume'),\n",
    "                                               gl.aggregate.SUM('low_volume'),\n",
    "                                               gl.aggregate.SUM('close_volume'),\n",
    "                                               gl.aggregate.SUM('volume'),\n",
    "                                              ]\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_d1_tmp = sf_eur_usd_d1_tmp.sort(['year','month','day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_d1_tmp = sf_eur_usd_d1_tmp.add_column(sf_eur_usd_d1_tmp['Sum of open_volume'] /sf_eur_usd_d1_tmp['Sum of volume'] ,'open')\n",
    "sf_eur_usd_d1_tmp = sf_eur_usd_d1_tmp.add_column(sf_eur_usd_d1_tmp['Sum of high_volume'] /sf_eur_usd_d1_tmp['Sum of volume'] ,'high')\n",
    "sf_eur_usd_d1_tmp = sf_eur_usd_d1_tmp.add_column(sf_eur_usd_d1_tmp['Sum of low_volume'] /sf_eur_usd_d1_tmp['Sum of volume'] ,'low')\n",
    "sf_eur_usd_d1_tmp = sf_eur_usd_d1_tmp.add_column(sf_eur_usd_d1_tmp['Sum of close_volume'] /sf_eur_usd_d1_tmp['Sum of volume'] ,'close')\n",
    "sf_eur_usd_d1_tmp = sf_eur_usd_d1_tmp.rename({'Sum of volume': 'volume'})\n",
    "sf_eur_usd_d1_tmp = sf_eur_usd_d1_tmp.remove_columns(['Sum of open_volume','Sum of high_volume','Sum of low_volume','Sum of close_volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_d1 = sf_eur_usd_d1_tmp\n",
    "\n",
    "# Use D1 to calculate features, h1 also could be used to calculate features with same logic\n",
    "sf_eur_usd_features = sf_eur_usd_d1 \n",
    "#sf_eur_usd_features = sf_eur_usd_h1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pip_unit = 10000\n",
    "close_list = sf_eur_usd_features ['close']\n",
    "def close_minus_n_time_unit(time_unit):\n",
    "    close_list = sf_eur_usd_features ['close']\n",
    "    if time_unit >=1:\n",
    "        close_minus_n_time_unit_list = close_list[0: time_unit].append(close_list[:-1*time_unit])\n",
    "    else:\n",
    "        print(\"close_minus_n_time_unit() parameter error!\")\n",
    "    return close_minus_n_time_unit_list\n",
    "\n",
    "close_minus_1 = close_minus_n_time_unit(1)\n",
    "close_minus_2 = close_minus_n_time_unit(2)\n",
    "close_minus_3 = close_minus_n_time_unit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_features = sf_eur_usd_features.add_column((close_list - close_minus_1) *pip_unit ,'pips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_pips_list = sf_eur_usd_features['pips'][1:].append(gl.SArray([0.0]))\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(predict_pips_list,'predict pips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_features = sf_eur_usd_features.add_column(predict_pips_list.apply(lambda x: 'sell' if x<0 else 'buy'),'predict action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_momentum(time_unit):\n",
    "    close_list = sf_eur_usd_features ['close']\n",
    "    close_minus_n = close_minus_n_time_unit(time_unit)\n",
    "    return close_list - close_minus_n\n",
    "\n",
    "def calculate_roc(time_unit):\n",
    "    close_list = sf_eur_usd_features ['close']\n",
    "    close_minus_n = close_minus_n_time_unit(time_unit)\n",
    "    return (close_list - close_minus_n)/close_minus_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_momentum(3),'momentum_3')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_momentum(4),'momentum_4')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_momentum(5),'momentum_5')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_momentum(8),'momentum_8')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_momentum(9),'momentum_9')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_momentum(10),'momentum_10')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(3),'roc_3')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(4),'roc_4')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(5),'roc_5')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(8),'roc_8')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(9),'roc_9')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(10),'roc_10')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(12),'roc_12')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(13),'roc_13')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(14),'roc_14')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_roc(15),'roc_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_fast_k_d(time_unit):\n",
    "    #Fast K 100 * [( C - L (n) ) / ( H (n) – L (n) )] . Use the data in same day as initial value\n",
    "    #L(n) means the lowest Low during the n-day period\n",
    "    #H(n) means the highest high during the n-day period\n",
    "    #http://investexcel.net/how-to-calculate-the-stochastic-oscillator/\n",
    "    high_python_list = list(sf_eur_usd_features ['high'])\n",
    "    low_python_list = list(sf_eur_usd_features ['low'])\n",
    "    high_high_list =[]\n",
    "    low_low_list=[]\n",
    "    for i in range(close_list.size()):\n",
    "        start = max(0,i-time_unit)\n",
    "        high_high = max(high_python_list[start:i+1])\n",
    "        high_high_list.append(high_high)\n",
    "        low_low = min(low_python_list[start:i+1])\n",
    "        low_low_list.append(low_low)\n",
    "    high_high_list_gl = gl.SArray(high_high_list)\n",
    "    low_low_list_gl = gl.SArray(low_low_list)\n",
    "    fast_k = 100 *(close_list - low_low_list_gl) / (high_high_list_gl - low_low_list_gl)\n",
    "    #remove n/a to zero\n",
    "    fast_k = fast_k.fillna(0)\n",
    "    fast_k_1 = fast_k[0:1].append(fast_k[:-1])\n",
    "    fast_k_2 = fast_k[0:2].append(fast_k[:-2])\n",
    "    fast_d = (fast_k + fast_k_1 + fast_k_2 ) /3\n",
    "    return fast_k, fast_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_k_d_3 = calculate_fast_k_d(3)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_3[0],'fast_k_3')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_3[1],'fast_d_3')\n",
    "fast_k_d_4 = calculate_fast_k_d(4)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_4[0],'fast_k_4')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_4[1],'fast_d_4')\n",
    "fast_k_d_5 = calculate_fast_k_d(5)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_5[0],'fast_k_5')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_5[1],'fast_d_5')\n",
    "fast_k_d_8 = calculate_fast_k_d(8)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_8[0],'fast_k_8')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_8[1],'fast_d_8')\n",
    "fast_k_d_9 = calculate_fast_k_d(9)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_9[0],'fast_k_9')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_9[1],'fast_d_9')\n",
    "fast_k_d_10 = calculate_fast_k_d(10)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_10[0],'fast_k_10')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fast_k_d_10[1],'fast_d_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate average price list\n",
    "\n",
    "def calculate_avg_price(price_list,time_unit):\n",
    "    price_list_python = list(price_list)\n",
    "    avg_price_list_python=[]\n",
    "    for i in range(price_list.size()):\n",
    "        start = max(0, i-time_unit)\n",
    "        avg_price = sum(price_list_python[start:i+1])/(i-start +1.0)\n",
    "        avg_price_list_python.append(avg_price)\n",
    "    return gl.SArray(avg_price_list_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weighted Closing Price\n",
    "def calculate_weighted_close_price(time_unit):\n",
    "    close_list = sf_eur_usd_features ['close']\n",
    "    avg_close_list = calculate_avg_price(close_list,15)\n",
    "    high_list = sf_eur_usd_features ['high']\n",
    "    avg_high_list = calculate_avg_price(high_list,15)\n",
    "    low_list = sf_eur_usd_features ['low']\n",
    "    avg_low_list = calculate_avg_price(low_list,15)\n",
    "    weighted_close_price_list = (avg_close_list * 2 + avg_high_list + avg_low_list) / 4.0\n",
    "    return weighted_close_price_list\n",
    "    \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_weighted_close_price(15),'weighted close price_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_william_r(time_unit):\n",
    "    #http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:williams_r\n",
    "    #%R = (Highest High - Close)/(Highest High - Lowest Low) * -100\n",
    "    # Lowest Low = lowest low for the look-back period\n",
    "    # Highest High = highest high for the look-back period\n",
    "    # %R is multiplied by -100 correct the inversion and move the decimal.\n",
    "    high_python_list = list(sf_eur_usd_features ['high'])\n",
    "    low_python_list = list(sf_eur_usd_features ['low'])\n",
    "    high_high_list =[]\n",
    "    low_low_list=[]\n",
    "    for i in range(close_list.size()):\n",
    "        start = max(0,i-time_unit)\n",
    "        high_high = max(high_python_list[start:i+1])\n",
    "        high_high_list.append(high_high)\n",
    "        low_low = min(low_python_list[start:i+1])\n",
    "        low_low_list.append(low_low)\n",
    "    high_high_list_gl = gl.SArray(high_high_list)\n",
    "    low_low_list_gl = gl.SArray(low_low_list)\n",
    "    \n",
    "    william_r = -100 *(high_high_list_gl - close_list ) / (high_high_list_gl - low_low_list_gl)\n",
    "    #remove n/a to zero\n",
    "    william_r = william_r.fillna(0)\n",
    "    return william_r\n",
    "\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_william_r(6),'william_r_6')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_william_r(7),'william_r_7')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_william_r(8),'william_r_8')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_william_r(9),'william_r_9')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_william_r(10),'william_r_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_william_a_d():\n",
    "    high_list = sf_eur_usd_features['high']\n",
    "    high_list_1 = high_list[0:1].append(high_list[:-1])\n",
    "    low_list = sf_eur_usd_features['low']\n",
    "    low_list_1 = low_list[0:1].append(low_list[:-1])\n",
    "    high_sf = gl.SFrame({'now':high_list,'yesterday':high_list_1})\n",
    "    low_sf = gl.SFrame({'now':low_list,'yesterday':low_list_1})\n",
    "    true_high = high_sf.apply(lambda x: max(x['now'],x['yesterday']))\n",
    "    true_low = low_sf.apply(lambda x: min(x['now'],x['yesterday']))\n",
    "    close_list = sf_eur_usd_features['close']\n",
    "    close_list_1 = close_list[0:1].append(close_list[:-1])\n",
    "    close_sf = gl.SFrame({'now_close':close_list,\n",
    "                          'yesterday_close':close_list_1, \n",
    "                          'true_high':true_high,\n",
    "                          'true_low':true_low})\n",
    "    today_a_d = close_sf.apply(\n",
    "                lambda x: ( (x['now_close'] - x['true_low'])  if (x['now_close'] - x['yesterday_close']) > 0 else \n",
    "                            (x['now_close'] - x['true_high']) if (x['now_close'] - x['yesterday_close']) < 0 else\n",
    "                            0)\n",
    "                )\n",
    "    today_a_d_1 = today_a_d[0:1].append(today_a_d[:-1])\n",
    "    william_a_d = today_a_d + today_a_d_1\n",
    "    return william_a_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_william_a_d(),'william_a_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_a_d_line(time_unit):\n",
    "    #http://www.metastock.com/Customer/Resources/TAAZ/?c=3&p=27\n",
    "    high_list = sf_eur_usd_features['high']\n",
    "    low_list = sf_eur_usd_features['low']\n",
    "    close_list = sf_eur_usd_features['close']\n",
    "    volume_list = sf_eur_usd_features['volume']\n",
    "    clv_list = ((close_list - low_list) - (high_list - close_list)) /(high_list - low_list) * volume_list\n",
    "    clv_list = clv_list.fillna(0)\n",
    "    clv_list_python = list(clv_list)\n",
    "    a_d_line_python = []\n",
    "    for i in range(close_list.size()):\n",
    "        start = max(0,i-time_unit)\n",
    "        a_d_value = sum(clv_list_python[start:i+1])\n",
    "        a_d_line_python.append(a_d_value)\n",
    "    a_d_line_gl = gl.SArray(a_d_line_python)\n",
    "    return a_d_line_gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_a_d_line(1),'adsoc_1')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_a_d_line(2),'adsoc_2')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_a_d_line(3),'adsoc_3')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_a_d_line(4),'adsoc_4')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_a_d_line(5),'adsoc_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sf_eur_usd_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_ema(time_unit):\n",
    "    #http://investexcel.net/how-to-calculate-macd-in-excel/\n",
    "    close_list = sf_eur_usd_features['close']\n",
    "    close_list_python = list(close_list)\n",
    "    ema_list=[]\n",
    "    for i in range(close_list.size()):\n",
    "        if i< time_unit: \n",
    "            ema_value = sum(close_list_python[0:i+1]) / float(i+1)\n",
    "            ema_list.append(ema_value)\n",
    "        else:\n",
    "            ema_value = (close_list_python[i] * (2.0 /(time_unit +1.0)) +\n",
    "                         ema_list[i-1] *(1.0-(2.0/(time_unit+1.0))) )\n",
    "            ema_list.append(ema_value)\n",
    "    return gl.SArray(ema_list)\n",
    "\n",
    "macd = calculate_ema(12) - calculate_ema(26)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(macd,'macd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_cci_20():\n",
    "    high_list = sf_eur_usd_features['high']\n",
    "    low_list = sf_eur_usd_features['low']\n",
    "    close_list = sf_eur_usd_features['close']\n",
    "    typical_price_list = (high_list + low_list + close_list) /3.0\n",
    "    typical_price_list_python  = list(typical_price_list)\n",
    "    sma_typical_price_list_python = []\n",
    "    cci_list_python =[]\n",
    "    for i in range(close_list.size()):\n",
    "        start = max(0, i-19)\n",
    "        sma_typical_price = sum(typical_price_list_python[start:i+1])/(i-start +1.0)\n",
    "        sma_typical_price_list_python.append(sma_typical_price)\n",
    "        std_typical_price = sum(\n",
    "            [abs(tmp_typical_price - sma_typical_price ) \n",
    "                for tmp_typical_price in sma_typical_price_list_python[start:i+1] \n",
    "            ]) /(i-start +1.0)\n",
    "        if std_typical_price == 0.0:\n",
    "            cci =0.0\n",
    "        else:\n",
    "            cci = ( (typical_price_list_python[i] - sma_typical_price ) /\n",
    "               (0.015*std_typical_price) )\n",
    "        cci_list_python.append(cci)\n",
    "    return gl.SArray(cci_list_python).astype(int)\n",
    "\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_cci_20(),'cci_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_Bollinger_Bands_20():\n",
    "    close_list = sf_eur_usd_features['close']\n",
    "    close_list_python = list(close_list)\n",
    "    std_close_list_python = []\n",
    "    sma_list_python = []\n",
    "    for i in range(close_list.size()):\n",
    "        start = max(0, i-19)\n",
    "        sma_close = sum(close_list_python[start:i+1])/ (i-start +1.0)\n",
    "        std_close = sum(\n",
    "            [abs(tmp_close_price - sma_close ) \n",
    "                for tmp_close_price in close_list_python[start:i+1] \n",
    "            ]) /(i-start +1.0)\n",
    "        std_close_list_python.append(std_close)\n",
    "        sma_list_python.append(sma_close)\n",
    "    sma_list = gl.SArray(sma_list_python)\n",
    "    std_close_list = gl.SArray(std_close_list_python)\n",
    "    return (sma_list - std_close_list *2), (sma_list + std_close_list *2)\n",
    "\n",
    "bb_list = calculate_Bollinger_Bands_20()\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(bb_list[0],'Bollinger_Bands_20_down')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(bb_list[1],'Bollinger_Bands_20_up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sf_eur_usd_features[['Bollinger_Bands_20_down','Bollinger_Bands_20_up','close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_heikin_ashi(time_unit):\n",
    "    #http://www.investopedia.com/articles/technical/04/092204.asp\n",
    "    high_list = sf_eur_usd_features['high']\n",
    "    high_list_avg =  calculate_avg_price(high_list,time_unit)\n",
    "    \n",
    "    low_list = sf_eur_usd_features['low']\n",
    "    low_list_avg =  calculate_avg_price(low_list,time_unit)\n",
    "    \n",
    "    close_list = sf_eur_usd_features['close']\n",
    "    close_list_avg =  calculate_avg_price(close_list,time_unit)\n",
    "    close_list_avg_1 = close_list_avg[0:1].append(close_list_avg[:-1])    \n",
    "    \n",
    "    open_list = sf_eur_usd_features['open']\n",
    "    open_list_avg =  calculate_avg_price(open_list,time_unit)\n",
    "    open_list_avg_1 = open_list_avg[0:1].append(open_list_avg[:-1])\n",
    "    \n",
    "    xclose_list = (open_list_avg + high_list_avg + low_list_avg + close_list_avg)/4.0\n",
    "    \n",
    "    xopen_list = (open_list_avg_1 + close_list_avg_1) /2.0\n",
    "    \n",
    "    sf_heikin_ashi = gl.SFrame({\n",
    "                                'high': high_list_avg,\n",
    "                                'low': low_list_avg,\n",
    "                                'xopen': xopen_list,\n",
    "                                'xclose': xclose_list\n",
    "                                 })\n",
    "    #(xhigh_list,xlow_list) = sf_heikin_ashi.apply(\n",
    "    #    lambda x: (max(x['high'],x['xopen'],x['xclose']), min(x['low'],x['xopen'],x['xclose'])))\n",
    "    xhigh_list = sf_heikin_ashi.apply(\n",
    "        lambda x: max(x['high'],x['xopen'],x['xclose']) )\n",
    "    xlow_list = sf_heikin_ashi.apply(\n",
    "        lambda x: min(x['low'],x['xopen'],x['xclose']) )\n",
    "    return xopen_list, xhigh_list, xlow_list, xclose_list\n",
    "\n",
    "heikin_ashi_lists_15 =calculate_heikin_ashi(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_eur_usd_features = sf_eur_usd_features.add_column(heikin_ashi_lists_15[0],'heikin_ashi_open')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(heikin_ashi_lists_15[1],'heikin_ashi_high')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(heikin_ashi_lists_15[2],'heikin_ashi_low')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(heikin_ashi_lists_15[3],'heikin_ashi_close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2day high/low average\n",
    "high_list = sf_eur_usd_features['high']\n",
    "high_list_1 = high_list[0:1].append(high_list[:-1])\n",
    "low_list = sf_eur_usd_features['low']\n",
    "low_list_1 = low_list[0:1].append(low_list[:-1])\n",
    "high_low_avg_2day = ( high_list + high_list_1 + low_list + low_list_1 ) /4.0\n",
    "high_avg_2day = ( high_list + high_list_1) /2.0\n",
    "low_avg_2day = ( low_list + low_list_1) /2.0\n",
    "high_low_avg_1day = ( high_list + low_list ) /2.0\n",
    "\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(high_low_avg_2day,'high_low_avg_2day')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(high_low_avg_1day,'high_low_avg_1day')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(high_avg_2day,'high_avg_2day')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(low_avg_2day,'low_avg_2day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_slope(time_unit):\n",
    "    close_list = sf_eur_usd_features['close']\n",
    "    close_list_n = close_list[0:time_unit].append(close_list[:-1*time_unit])\n",
    "    slope_list = (close_list - close_list_n) / time_unit\n",
    "    return slope_list\n",
    "\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(3),'close_slope_3')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(4),'close_slope_4')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(5),'close_slope_5')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(8),'close_slope_8')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(10),'close_slope_10')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(12),'close_slope_12')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(15),'close_slope_15')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(20),'close_slope_20')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(25),'close_slope_25')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(calculate_slope(30),'close_slope_30')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_garch_1_1():\n",
    "    #http://investexcel.net/garch-excel/\n",
    "    close_list = sf_eur_usd_features['close']\n",
    "    close_list_1 = close_list[0:1].append(close_list[:-1])\n",
    "    Residual_list =  close_list - close_list_1\n",
    "    suqred_Residual_list = Residual_list * Residual_list\n",
    "    lagged_1_suqred_Residual_list = suqred_Residual_list[0:1].append(suqred_Residual_list[:-1])\n",
    "    unconditional_variance = close_list.var()\n",
    "    w = 0.00000397106501352437\n",
    "    alpha = 0.0824292201426092\n",
    "    beta = 0.874056639703639\n",
    "    conditional_variance_list =[]\n",
    "    sqrt_conditional_variance_list =[]\n",
    "    log_like_conditional_variance_list =[]\n",
    "    for i in range(close_list.size()):\n",
    "        if i==0:\n",
    "            conditional_variance = unconditional_variance\n",
    "        else:\n",
    "            conditional_variance = w + alpha*lagged_1_suqred_Residual_list[i] + beta*conditional_variance_list[i-1]\n",
    "        \n",
    "        conditional_variance_list.append(conditional_variance)\n",
    "        \n",
    "        sqrt_conditional_variance_list.append(\n",
    "            math.sqrt(conditional_variance)\n",
    "        )\n",
    "        \n",
    "        log_like_conditional_variance_list.append(\n",
    "            math.log(\n",
    "                (1.0/math.sqrt(2*3.1415927*conditional_variance))*\n",
    "                  math.exp(-0.5*suqred_Residual_list[i]/conditional_variance)\n",
    "            )\n",
    "        )\n",
    "    return gl.SArray(log_like_conditional_variance_list), gl.SArray(sqrt_conditional_variance_list)\n",
    "\n",
    "garch_1_1_list = calculate_garch_1_1()\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(garch_1_1_list[0],'garch_1_1_log_like')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(garch_1_1_list[1],'garch_1_1_sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_fft(time_unit):\n",
    "    #Only reserve a0,a1,b1,a2,b2 factors for each sequence of fft\n",
    "    #The data which is not long enough to generate time_unit length of data will be avoid to calculate\n",
    "    if time_unit <5:\n",
    "        print \"calculate_fft: time_unit parameter is too small.\"\n",
    "        exit(-1)\n",
    "    close_list = sf_eur_usd_features['close']\n",
    "    close_list_python = list(sf_eur_usd_features['close'])\n",
    "    fft_a0 =[]\n",
    "    fft_a1 =[]\n",
    "    fft_b1 =[]\n",
    "    fft_a2 =[]\n",
    "    fft_b2 =[]\n",
    "    for i in range(close_list.size()):\n",
    "        if i-time_unit +1 < 0:\n",
    "            a0=a1=b1=a2=b2=0.0\n",
    "        else:\n",
    "            start = i-time_unit +1\n",
    "            fft_source = close_list_python[start:i+1]\n",
    "            fft_trans = np.fft.fft(fft_source)\n",
    "            a0 = fft_trans[0].real\n",
    "            a1 = fft_trans[1].real\n",
    "            a2 = fft_trans[2].real\n",
    "            b1 = fft_trans[1].imag\n",
    "            b2 = fft_trans[2].imag\n",
    "        fft_a0.append(a0)\n",
    "        fft_a1.append(a1)\n",
    "        fft_b1.append(b1)\n",
    "        fft_a2.append(a2)\n",
    "        fft_b2.append(b2)\n",
    "    return gl.SArray(fft_a0),gl.SArray(fft_a1),gl.SArray(fft_b1),gl.SArray(fft_a2),gl.SArray(fft_b2)\n",
    "\n",
    "\n",
    "fft_5= calculate_fft(5)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_5[0],'fft_5_a0')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_5[1],'fft_5_a1')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_5[2],'fft_5_b1')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_5[3],'fft_5_a2')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_5[4],'fft_5_b2')\n",
    "\n",
    "fft_10= calculate_fft(10)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_10[0],'fft_10_a0')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_10[1],'fft_10_a1')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_10[2],'fft_10_b1')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_10[3],'fft_10_a2')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_10[4],'fft_10_b2')\n",
    "\n",
    "fft_20= calculate_fft(20)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_20[0],'fft_20_a0')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_20[1],'fft_20_a1')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_20[2],'fft_20_b1')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_20[3],'fft_20_a2')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_20[4],'fft_20_b2')\n",
    "\n",
    "fft_30= calculate_fft(30)\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_30[0],'fft_30_a0')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_30[1],'fft_30_a1')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_30[2],'fft_30_b1')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_30[3],'fft_30_a2')\n",
    "sf_eur_usd_features = sf_eur_usd_features.add_column(fft_30[4],'fft_30_b2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_time_serials_windows(feature_array,columne_base_name,time_unit):\n",
    "    # generate history time-serials features \n",
    "    sf_ts_window_feature = gl.SFrame({columne_base_name:feature_array})\n",
    "    for i in range(1,time_unit+1):\n",
    "        column_name = columne_base_name + '_'+str(i)\n",
    "        column_data = feature_array[0:i].append(feature_array[:-i])\n",
    "        sf_ts_window_feature.add_column(column_data,name=column_name)\n",
    "    return sf_ts_window_feature\n",
    "\n",
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['momentum_3'],'momentum_3',60)\n",
    "sf_eur_usd_features.remove_column('momentum_3')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)\n",
    "\n",
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['momentum_4'],'momentum_4',60)\n",
    "sf_eur_usd_features.remove_column('momentum_4')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)\n",
    "\n",
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['momentum_5'],'momentum_5',60)\n",
    "sf_eur_usd_features.remove_column('momentum_5')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)\n",
    "\n",
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['momentum_8'],'momentum_8',60)\n",
    "sf_eur_usd_features.remove_column('momentum_8')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)\n",
    "\n",
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['momentum_9'],'momentum_9',60)\n",
    "sf_eur_usd_features.remove_column('momentum_9')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)\n",
    "\n",
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['momentum_10'],'momentum_10',60)\n",
    "sf_eur_usd_features.remove_column('momentum_10')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['close'],'close',60)\n",
    "sf_eur_usd_features.remove_column('close')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['volume'],'volume',60)\n",
    "sf_eur_usd_features.remove_column('volume')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['garch_1_1_log_like'],'garch_1_1_log_like',60)\n",
    "sf_eur_usd_features.remove_column('garch_1_1_log_like')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['garch_1_1_sqrt'],'garch_1_1_sqrt',60)\n",
    "sf_eur_usd_features.remove_column('garch_1_1_sqrt')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)\n",
    "\n",
    "sf_1 = generate_time_serials_windows(sf_eur_usd_features['adsoc_1'],'adsoc_1',60)\n",
    "sf_eur_usd_features.remove_column('adsoc_1')                            \n",
    "sf_eur_usd_features = sf_eur_usd_features.add_columns(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_column= 'predict action'\n",
    "features_columns= ['volume','open','high','low','close',\n",
    " 'momentum_3','momentum_4','momentum_5','momentum_8','momentum_9','momentum_10',\n",
    " 'roc_3','roc_4','roc_5','roc_8','roc_9','roc_10','roc_12','roc_13','roc_14','roc_15',\n",
    " 'fast_k_3','fast_d_3','fast_k_4','fast_d_4','fast_k_5','fast_d_5','fast_k_8','fast_d_8','fast_k_9','fast_d_9','fast_k_10','fast_d_10',\n",
    " 'weighted close price_15','william_r_6','william_r_7','william_r_8','william_r_9','william_r_10',\n",
    " 'william_a_d','adsoc_1','adsoc_2','adsoc_3','adsoc_4','adsoc_5',\n",
    " 'macd','cci_20','Bollinger_Bands_20_down','Bollinger_Bands_20_up',\n",
    " 'heikin_ashi_open','heikin_ashi_high','heikin_ashi_low','heikin_ashi_close',\n",
    " 'high_low_avg_2day','high_low_avg_1day','high_avg_2day','low_avg_2day',\n",
    " 'close_slope_3','close_slope_4','close_slope_5','close_slope_8','close_slope_10','close_slope_12','close_slope_15',\n",
    " 'close_slope_20','close_slope_25','close_slope_30',\n",
    " 'garch_1_1_log_like','garch_1_1_sqrt',\n",
    " 'fft_5_a0','fft_5_a1','fft_5_b1','fft_5_a2','fft_5_b2',\n",
    " 'fft_10_a0','fft_10_a1','fft_10_b1','fft_10_a2','fft_10_b2',\n",
    " 'fft_20_a0', 'fft_20_a1','fft_20_b1','fft_20_a2','fft_20_b2',\n",
    " 'fft_30_a0','fft_30_a1','fft_30_b1','fft_30_a2','fft_30_b2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: The following methods are available for this type of problem.\n",
      "PROGRESS: LogisticClassifier, SVMClassifier\n",
      "PROGRESS: The returned model will be chosen according to validation accuracy.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 4020\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 549\n",
      "PROGRESS: Number of unpacked features : 549\n",
      "PROGRESS: Number of coefficients    : 550\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000011  | 0.139480     | 0.511194          | 0.543689            |\n",
      "PROGRESS: | 2         | 9        | 5.000000  | 0.232855     | 0.525871          | 0.475728            |\n",
      "PROGRESS: | 3         | 10       | 5.000000  | 0.279489     | 0.532587          | 0.432039            |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.348166     | 0.538060          | 0.500000            |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.395483     | 0.548507          | 0.504854            |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.443750     | 0.547264          | 0.490291            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 4020\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 549\n",
      "PROGRESS: Number of unpacked features : 549\n",
      "PROGRESS: Number of coefficients    : 550\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 7        | 0.000051  | 0.147656     | 0.511194          | 0.543689            |\n",
      "PROGRESS: | 2         | 12       | 0.124990  | 0.281378     | 0.511194          | 0.543689            |\n",
      "PROGRESS: | 3         | 13       | 0.124990  | 0.327889     | 0.511194          | 0.543689            |\n",
      "PROGRESS: | 4         | 14       | 0.124990  | 0.374183     | 0.510945          | 0.543689            |\n",
      "PROGRESS: | 5         | 15       | 0.124990  | 0.423491     | 0.511194          | 0.543689            |\n",
      "PROGRESS: | 6         | 16       | 0.124990  | 0.469900     | 0.510697          | 0.543689            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: Model selection based on validation accuracy:\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: LogisticClassifier              : 0.558252\n",
      "PROGRESS: SVMClassifier                   : 0.519417\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: Selecting LogisticClassifier based on validation set performance.\n"
     ]
    }
   ],
   "source": [
    "features_columns= ['volume','open','high','low','close']\n",
    "features_columns= [ 'momentum_3','momentum_4','momentum_5','momentum_8','momentum_9','momentum_10',\n",
    " 'roc_3','roc_4','roc_5','roc_8','roc_9','roc_10','roc_12','roc_13','roc_14','roc_15']\n",
    "def generate_feature_column_names(columne_base_name,time_unit):\n",
    "    column_names =[columne_base_name]\n",
    "    for i in range(1,time_unit+1):\n",
    "        column_name = columne_base_name + '_'+str(i)\n",
    "        column_names.append(column_name)\n",
    "    return column_names\n",
    "\n",
    "features_columns= []\n",
    "features_columns.extend(generate_feature_column_names('momentum_3',60))\n",
    "features_columns.extend(generate_feature_column_names('momentum_4',60))\n",
    "features_columns.extend(generate_feature_column_names('momentum_5',60))\n",
    "features_columns.extend(generate_feature_column_names('momentum_8',60))\n",
    "features_columns.extend(generate_feature_column_names('momentum_9',60))\n",
    "features_columns.extend(generate_feature_column_names('momentum_10',60))\n",
    "features_columns.extend(generate_feature_column_names('close',60))\n",
    "#features_columns.extend(generate_feature_column_names('volume',60))\n",
    "features_columns.extend(generate_feature_column_names('garch_1_1_log_like',60))\n",
    "features_columns.extend(generate_feature_column_names('garch_1_1_sqrt',60))\n",
    "#features_columns.extend(generate_feature_column_names('adsoc_1',60))\n",
    "\n",
    "model =  gl.classifier.create(sf_eur_usd_features, \n",
    "                              target=target_column,\n",
    "                              features=features_columns)\n",
    "results = model.evaluate(sf_eur_usd_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5655466161855183, 'confusion_matrix': Columns:\n",
       " \ttarget_label\tint\n",
       " \tpredicted_label\tint\n",
       " \tcount\tint\n",
       " \n",
       " Rows: 4\n",
       " \n",
       " Data:\n",
       " +--------------+-----------------+-------+\n",
       " | target_label | predicted_label | count |\n",
       " +--------------+-----------------+-------+\n",
       " |      1       |        0        |  1143 |\n",
       " |      0       |        0        |  1473 |\n",
       " |      0       |        1        |  693  |\n",
       " |      1       |        1        |  917  |\n",
       " +--------------+-----------------+-------+\n",
       " [4 rows x 3 columns]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_len = 15\n",
    "#test_day = -3 #0 today, -1 yesterday\n",
    "def predict_profit(start_date_index,end_date_index):#0 today, -1 yesterday\n",
    "    for test_day in range(start_date_index,end_date_index +1):    \n",
    "        #train and valid are used for model training with parameter search\n",
    "        train = sf_eur_usd_features[:test_day -1 - valid_len]\n",
    "        valid = sf_eur_usd_features[test_day -1 - valid_len:test_day -1]\n",
    "        \n",
    "        #Test are for prediction\n",
    "        if test_day ==0:\n",
    "            test = sf_eur_usd_features[-1:] #Only one date of data\n",
    "        else:\n",
    "            test = sf_eur_usd_features[test_day -1 :test_day] #Only one date of data\n",
    "            \n",
    "        params = {'target': target_column,\n",
    "                  'features': [features_columns]\n",
    "                 }\n",
    "        \n",
    "        ## SVM classifier\n",
    "        j_svm = gl.random_search.create((train, valid), \n",
    "                                 gl.svm_classifier.create, \n",
    "                                 params)\n",
    "        results_svm= j_svm.get_results()\n",
    "        model_svm_id = results_svm[results_svm['validation_accuracy'].argmax()]['model_id']\n",
    "        model_svm = j_svm.get_models()[model_svm_id]\n",
    "        accuracy_svm = gl.SArray([\n",
    "                results_svm['validation_accuracy'].max()\n",
    "                ])\n",
    "\n",
    "        prediction_svm = gl.SArray(model_svm.classify(test)['class'])\n",
    "        \n",
    "        ## GBM classifier\n",
    "\n",
    "        j_gbm = gl.random_search.create((train, valid), \n",
    "                                     gl.boosted_trees_classifier.create, \n",
    "                                     params)\n",
    "        results_gbm= j_gbm.get_results()\n",
    "        model_gbm_id = results_gbm[results_gbm['validation_accuracy'].argmax()]['model_id']\n",
    "        #print results_gbm\n",
    "        model_gbm = j_gbm.get_models()[model_gbm_id]\n",
    "        accuracy_gbm = gl.SArray([\n",
    "                results_gbm['validation_accuracy'].max()\n",
    "                ])\n",
    "        prediction_gbm = gl.SArray(model_gbm.classify(test)['class'])\n",
    "\n",
    "        ## Logistic classifier\n",
    "\n",
    "        j_logistic = gl.random_search.create((train, valid), \n",
    "                                     gl.logistic_classifier.create, \n",
    "                                     params)\n",
    "        results_logistic= j_logistic.get_results()\n",
    "        model_logistic_id = results_logistic[results_logistic['validation_accuracy'].argmax()]['model_id']\n",
    "        model_logistic = j_logistic.get_models()[model_logistic_id]\n",
    "        accuracy_logistic = gl.SArray([\n",
    "                results_logistic['validation_accuracy'].max()\n",
    "                ])\n",
    "        prediction_logistic = gl.SArray(model_logistic.classify(test)['class'])\n",
    "        \n",
    "        ## Neural Network classifier\n",
    "        j_neuralnet = gl.random_search.create((train, valid), \n",
    "                                     gl.neuralnet_classifier.create, \n",
    "                                     params)\n",
    "        results_neuralnet= j_neuralnet.get_results()\n",
    "        model_neuralnet_id = results_neuralnet[results_neuralnet['validation_accuracy'].argmax()]['model_id']\n",
    "        model_neuralnet = j_neuralnet.get_models()[model_neuralnet_id]\n",
    "        accuracy_neuralnet = gl.SArray([\n",
    "                results_neuralnet['validation_accuracy'].max()\n",
    "                ])\n",
    "        #print model_neuralnet\n",
    "        prediction_neuralnet = gl.SArray(model_neuralnet.classify(test)['class'])\n",
    "        \n",
    "        \n",
    "        test = test.add_column( prediction_svm,'svm_estimate')\n",
    "        test = test.add_column( accuracy_svm,'svm_accuracy')\n",
    "        \n",
    "        test = test.add_column( prediction_gbm,'gbm_estimate')\n",
    "        test = test.add_column( accuracy_gbm,'gbm_accuracy')\n",
    "        \n",
    "        test = test.add_column( prediction_logistic,'logistic_estimate')\n",
    "        test = test.add_column( accuracy_logistic,'logistic_accuracy')\n",
    "        \n",
    "        test = test.add_column( prediction_neuralnet,'neuralnet_estimate')\n",
    "        test = test.add_column( accuracy_neuralnet,'neuralnet_accuracy')\n",
    "        \n",
    "        if test_day == start_date_index: # For the first element\n",
    "            results_sf = test\n",
    "        else: #From second element, append it to the results\n",
    "            results_sf= results_sf.append(test)\n",
    "        print test_day\n",
    "        results_sf[\n",
    "            ['date','predict action',\n",
    "             'svm_estimate','svm_accuracy',\n",
    "             'gbm_estimate','gbm_accuracy',\n",
    "             'logistic_estimate','logistic_accuracy',\n",
    "             'neuralnet_estimate','neuralnet_accuracy']\n",
    "        ].show()\n",
    "    return results_sf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results_sf =  predict_profit(-300,0)\n",
    "#results_sf[['date','predict action','svm_estimate','gbm_estimate','logistic_estimate','neuralnet_estimate']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results_sf[['date','predict action','svm_estimate','gbm_estimate','logistic_estimate','neuralnet_estimate']].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
